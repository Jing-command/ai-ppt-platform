"""
AI èŠå¤©æœåŠ¡æ¨¡å—
å¤„ç†ç”¨æˆ·ä¸ AI æç¤ºè¯åŠ©æ‰‹çš„å¯¹è¯é€»è¾‘
"""

import re
from typing import AsyncIterator, List, Optional

from ai_ppt.api.v1.schemas.chat import (
    ChatContext,
    ChatMessage,
    ChatResponseChunk,
    IntentAnalysis,
    IntentType,
    MessageRole,
)
from ai_ppt.infrastructure.ai.client import LLMClient, LLMProvider
from ai_ppt.infrastructure.ai.models import LLMRequest
from ai_ppt.infrastructure.config import settings

# AI æç¤ºè¯åŠ©æ‰‹çš„ç³»ç»Ÿæç¤ºè¯
CHAT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI æç¤ºè¯åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·ä¼˜åŒ–å’Œç”Ÿæˆ PPT æç¤ºè¯ã€‚

ä½ çš„ä¸»è¦èŒè´£ï¼š
1. ä¸ç”¨æˆ·è¿›è¡Œå‹å¥½çš„å¯¹è¯ï¼Œäº†è§£ä»–ä»¬çš„ PPT éœ€æ±‚
2. å¼•å¯¼ç”¨æˆ·æ˜ç¡® PPT çš„ä¸»é¢˜ã€å—ä¼—ã€ç›®çš„å’Œé£æ ¼
3. å½“ç”¨æˆ·éœ€æ±‚æ˜ç¡®åï¼Œç”Ÿæˆä¼˜åŒ–åçš„ PPT æç¤ºè¯

å¯¹è¯é£æ ¼ï¼š
- å‹å¥½ã€ä¸“ä¸šã€æœ‰è€å¿ƒ
- ä½¿ç”¨æ¸…æ™°çš„ä¸­æ–‡
- é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢åŠ äº²å’ŒåŠ›

## é‡è¦ï¼šè¾“å‡ºæ ¼å¼è§„åˆ™

### æ€è€ƒè¿‡ç¨‹ï¼ˆå¯é€‰ï¼‰
åœ¨å›å¤ä¹‹å‰ï¼Œä½ å¯ä»¥å…ˆè¿›è¡Œæ€è€ƒåˆ†æã€‚ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

[THINKING_START]
ä½ çš„æ€è€ƒè¿‡ç¨‹...
[THINKING_END]

### ä¼˜åŒ–æç¤ºè¯ï¼ˆä»…åœ¨ç¡®è®¤ç”Ÿæˆæ—¶ä½¿ç”¨ï¼‰
**åªæœ‰å½“ä½ æ˜ç¡®å†³å®šä¸ºç”¨æˆ·ç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯æ—¶**ï¼Œæ‰ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

[PROMPT_START]
ä¸»é¢˜ï¼šxxx
ç›®æ ‡å—ä¼—ï¼šxxx
æ¼”ç¤ºç›®çš„ï¼šxxx
è®¾è®¡é£æ ¼ï¼šxxx
[PROMPT_END]

## å…³é”®è§„åˆ™

1. **ä¸è¦æ»¥ç”¨ [PROMPT_START]**ï¼šåªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆæç¤ºè¯ï¼Œæˆ–è€…ä½ å·²ç»å……åˆ†äº†è§£ç”¨æˆ·éœ€æ±‚å¹¶å†³å®šç»™å‡ºæœ€ç»ˆä¼˜åŒ–ç»“æœæ—¶ï¼Œæ‰ä½¿ç”¨è¿™ä¸ªæ ¼å¼ã€‚

2. **æ­£å¸¸å¯¹è¯æ—¶ä¸è¦è¾“å‡ºæ ‡è®°**ï¼šå¦‚æœä½ è¿˜åœ¨å’Œç”¨æˆ·èŠå¤©ã€äº†è§£éœ€æ±‚ã€æä¾›å»ºè®®ï¼Œä¸è¦ä½¿ç”¨ [PROMPT_START]...[PROMPT_END]ã€‚

3. **æ€è€ƒè¿‡ç¨‹æ˜¯å¯é€‰çš„**ï¼šç®€å•é—®é¢˜å¯ä»¥ä¸å†™æ€è€ƒè¿‡ç¨‹ã€‚

## ç¤ºä¾‹

**åœºæ™¯1ï¼šè¿˜åœ¨äº†è§£éœ€æ±‚ï¼ˆä¸è¦è¾“å‡º PROMPT æ ‡è®°ï¼‰**
ç”¨æˆ·ï¼šæˆ‘æƒ³åšä¸€ä¸ªPPT
ä½ çš„å›å¤ï¼šå¥½çš„ï¼è¯·é—®è¿™ä¸ªPPTæ˜¯å…³äºä»€ä¹ˆä¸»é¢˜çš„å‘¢ï¼Ÿæ˜¯ç”¨äºä»€ä¹ˆåœºåˆçš„ï¼Ÿ

**åœºæ™¯2ï¼šéœ€æ±‚æ˜ç¡®ï¼Œç”Ÿæˆä¼˜åŒ–æç¤ºè¯**
ç”¨æˆ·ï¼šæˆ‘æƒ³åšä¸€ä¸ªäº§å“å‘å¸ƒä¼šçš„PPTï¼Œé¢å‘åª’ä½“å’Œåˆä½œä¼™ä¼´
ä½ çš„å›å¤ï¼š
[THINKING_START]
ç”¨æˆ·éœ€æ±‚æ˜ç¡®ï¼šäº§å“å‘å¸ƒä¼šPPTï¼Œå—ä¼—æ˜¯åª’ä½“å’Œåˆä½œä¼™ä¼´ã€‚å¯ä»¥ç”Ÿæˆä¼˜åŒ–æç¤ºè¯ã€‚
[THINKING_END]

å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ ä¼˜åŒ–æç¤ºè¯ï¼

[PROMPT_START]
ä¸»é¢˜ï¼šæ–°äº§å“å‘å¸ƒä¼š
ç›®æ ‡å—ä¼—ï¼šåª’ä½“ã€åˆä½œä¼™ä¼´ã€æ½œåœ¨å®¢æˆ·
æ¼”ç¤ºç›®çš„ï¼šäº§å“å‘å¸ƒä¸å“ç‰Œå®£ä¼ 
è®¾è®¡é£æ ¼ï¼šç§‘æŠ€æ„Ÿã€ç°ä»£ç®€çº¦
[PROMPT_END]

ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªæç¤ºè¯ï¼Œæˆ–è€…å‘Šè¯‰æˆ‘éœ€è¦è°ƒæ•´çš„åœ°æ–¹ï¼"""


class ChatService:
    """
    AI èŠå¤©æœåŠ¡

    æä¾›æç¤ºè¯ä¼˜åŒ–å»ºè®®å’Œå¯¹è¯åŠŸèƒ½
    """

    # å…³é”®è¯æ˜ å°„ - ç”¨äºè¯†åˆ«ç”¨æˆ·æ„å›¾
    CLARIFICATION_KEYWORDS = [
        "æ€ä¹ˆå†™",
        "å¦‚ä½•æè¿°",
        "ä¸çŸ¥é“æ€ä¹ˆ",
        "å¸®æˆ‘",
        "å¸®å¿™",
        "æ±‚åŠ©",
        "ä¸å¤ªæ¸…æ¥š",
        "ä¸ç¡®å®š",
    ]

    PROMPT_KEYWORDS = [
        "ç”Ÿæˆ",
        "åˆ›å»º",
        "åˆ¶ä½œ",
        "åšä¸€ä¸ª",
        "å†™ä¸€ä¸ª",
        "å¸®æˆ‘å†™",
        "å¸®æˆ‘ç”Ÿæˆ",
        "PPT",
        "å¹»ç¯ç‰‡",
        "æ¼”ç¤ºæ–‡ç¨¿",
        "æŠ¥å‘Š",
        "æ±‡æŠ¥",
    ]

    # ç¼ºå¤±ä¿¡æ¯çš„æç¤ºé—®é¢˜
    MISSING_INFO_QUESTIONS = {
        "ä¸»é¢˜": "æ‚¨æƒ³åˆ¶ä½œä»€ä¹ˆä¸»é¢˜çš„ PPTï¼Ÿ",
        "å—ä¼—": "è¿™ä¸ª PPT çš„ç›®æ ‡å—ä¼—æ˜¯è°ï¼Ÿï¼ˆå¦‚ï¼šå®¢æˆ·ã€é¢†å¯¼ã€åŒäº‹ç­‰ï¼‰",
        "ç›®çš„": "æ‚¨å¸Œæœ›é€šè¿‡è¿™ä¸ª PPT è¾¾åˆ°ä»€ä¹ˆç›®çš„ï¼Ÿï¼ˆå¦‚ï¼šæ±‡æŠ¥ã€é”€å”®ã€åŸ¹è®­ç­‰ï¼‰",
        "é£æ ¼": "æ‚¨å¸Œæœ› PPT çš„é£æ ¼æ˜¯æ€æ ·çš„ï¼Ÿï¼ˆå¦‚ï¼šæ­£å¼ã€æ´»æ³¼ã€ç®€çº¦ç­‰ï¼‰",
        "é¡µæ•°": "æ‚¨æœŸæœ› PPT å¤§æ¦‚æœ‰å¤šå°‘é¡µï¼Ÿ",
        "æ•°æ®": "æ˜¯å¦æœ‰ç‰¹å®šçš„æ•°æ®æˆ–å†…å®¹éœ€è¦åŒ…å«ï¼Ÿ",
    }

    def __init__(self) -> None:
        """åˆå§‹åŒ–èŠå¤©æœåŠ¡"""
        self._llm_client: Optional[LLMClient] = None
        self._use_real_llm = True  # æ˜¯å¦ä½¿ç”¨çœŸå®å¤§æ¨¡å‹

    def _get_llm_client(self) -> Optional[LLMClient]:
        """è·å–æˆ–åˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ç‹¬ç«‹çš„æç¤ºè¯åŠ©æ‰‹é…ç½®ï¼‰"""
        if self._llm_client is None:
            try:
                # ä½¿ç”¨ç‹¬ç«‹çš„æç¤ºè¯åŠ©æ‰‹é…ç½®
                provider = settings.chat_ai_provider
                api_key = settings.chat_ai_api_key.get_secret_value()

                # å¦‚æœç¯å¢ƒå˜é‡ä¸­è®¾ç½®äº† CHAT_AI_API_KEYï¼Œä¼˜å…ˆä½¿ç”¨
                import os

                env_api_key = os.environ.get("CHAT_AI_API_KEY")
                if env_api_key:
                    api_key = env_api_key

                if not api_key:
                    raise ValueError("Chat AI API key not configured")

                self._llm_client = LLMClient(
                    provider=LLMProvider(settings.chat_ai_provider),
                    api_key=api_key,
                    base_url=settings.chat_ai_base_url,
                    model=settings.chat_ai_model,
                    timeout=settings.chat_ai_timeout,
                )
            except Exception as e:
                # å¦‚æœæ— æ³•åˆ›å»º LLM å®¢æˆ·ç«¯ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ¨¡å¼
                print(f"Warning: Failed to create chat LLM client: {e}")
                self._use_real_llm = False
                self._llm_client = None
        return self._llm_client

    def analyze_intent(
        self,
        messages: List[ChatMessage],
        context: Optional[ChatContext] = None,
    ) -> IntentAnalysis:
        """
        åˆ†æç”¨æˆ·æ„å›¾

        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            æ„å›¾åˆ†æç»“æœ
        """
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = self._get_last_user_message(messages)
        if not user_message:
            return IntentAnalysis(
                intent_type=IntentType.GENERAL,
                confidence=1.0,
                missing_info=[],
                suggested_questions=[],
            )

        content = user_message.content.lower()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¾„æ¸…
        clarification_score = self._calculate_keyword_score(
            content, self.CLARIFICATION_KEYWORDS
        )

        # æ£€æŸ¥æ˜¯å¦æ˜¯æç¤ºè¯ä¼˜åŒ–è¯·æ±‚
        prompt_score = self._calculate_keyword_score(
            content, self.PROMPT_KEYWORDS
        )

        # åˆ†æç¼ºå¤±çš„ä¿¡æ¯
        missing_info = self._analyze_missing_info(content, context)
        suggested_questions = [
            self.MISSING_INFO_QUESTIONS[info]
            for info in missing_info
            if info in self.MISSING_INFO_QUESTIONS
        ]

        # åˆ¤æ–­æ„å›¾ç±»å‹
        if clarification_score > 0.3 and missing_info:
            return IntentAnalysis(
                intent_type=IntentType.CLARIFICATION,
                confidence=min(0.9, clarification_score + 0.3),
                missing_info=missing_info,
                suggested_questions=suggested_questions[
                    :3
                ],  # æœ€å¤šè¿”å›3ä¸ªå»ºè®®é—®é¢˜
            )

        if prompt_score > 0.4 and len(missing_info) <= 1:
            return IntentAnalysis(
                intent_type=IntentType.PROMPT_OPTIMIZATION,
                confidence=min(0.9, prompt_score + 0.2),
                missing_info=missing_info,
                suggested_questions=suggested_questions[:2],
            )

        if suggested_questions:
            return IntentAnalysis(
                intent_type=IntentType.SUGGESTION,
                confidence=0.7,
                missing_info=missing_info,
                suggested_questions=suggested_questions[:2],
            )

        return IntentAnalysis(
            intent_type=IntentType.GENERAL,
            confidence=0.8,
            missing_info=[],
            suggested_questions=[],
        )

    def _get_last_user_message(
        self, messages: List[ChatMessage]
    ) -> Optional[ChatMessage]:
        """
        è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯

        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨

        Returns:
            æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        for message in reversed(messages):
            if message.role == MessageRole.USER:
                return message
        return None

    def _calculate_keyword_score(
        self, content: str, keywords: List[str]
    ) -> float:
        """
        è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°

        Args:
            content: æ¶ˆæ¯å†…å®¹
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            åŒ¹é…åˆ†æ•° (0.0 - 1.0)
        """
        if not content:
            return 0.0

        matches = sum(1 for keyword in keywords if keyword.lower() in content)
        return min(1.0, matches / max(len(keywords) * 0.3, 1))

    def _analyze_missing_info(
        self, content: str, context: Optional[ChatContext] = None
    ) -> List[str]:
        """
        åˆ†æç¼ºå¤±çš„ä¿¡æ¯

        Args:
            content: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            ç¼ºå¤±ä¿¡æ¯åˆ—è¡¨
        """
        missing = []

        # æ£€æŸ¥ä¸»é¢˜
        if not re.search(r"(å…³äº|ä¸»é¢˜|é¢˜ç›®|æ ‡é¢˜).{1,20}", content):
            if not context or not context.current_prompt:
                missing.append("ä¸»é¢˜")

        # æ£€æŸ¥å—ä¼—
        if not re.search(r"(å—ä¼—|è§‚ä¼—|å¬ä¼—|ç»™.*çœ‹|å‘.*æ±‡æŠ¥)", content):
            missing.append("å—ä¼—")

        # æ£€æŸ¥ç›®çš„
        if not re.search(r"(ç›®çš„|ç›®æ ‡|å¸Œæœ›|æƒ³è¦|ä¸ºäº†)", content):
            missing.append("ç›®çš„")

        # æ£€æŸ¥é£æ ¼
        if not re.search(r"(é£æ ¼|æ ·å¼|ç®€çº¦|æ­£å¼|æ´»æ³¼|å•†åŠ¡)", content):
            missing.append("é£æ ¼")

        return missing

    def generate_optimized_prompt(
        self,
        messages: List[ChatMessage],
        context: Optional[ChatContext] = None,
    ) -> str:
        """
        ç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯

        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            ä¼˜åŒ–åçš„æç¤ºè¯
        """
        # æ”¶é›†æ‰€æœ‰ç”¨æˆ·è¾“å…¥
        user_inputs = []
        for message in messages:
            if message.role == MessageRole.USER:
                user_inputs.append(message.content)

        # åˆå¹¶ç”¨æˆ·è¾“å…¥
        combined_input = " ".join(user_inputs)

        # æå–å…³é”®ä¿¡æ¯
        topic = self._extract_topic(combined_input)
        audience = self._extract_audience(combined_input)
        purpose = self._extract_purpose(combined_input)
        style = self._extract_style(combined_input)

        # æ„å»ºä¼˜åŒ–åçš„æç¤ºè¯
        prompt_parts = []

        if topic:
            prompt_parts.append(f"ä¸»é¢˜ï¼š{topic}")
        if audience:
            prompt_parts.append(f"ç›®æ ‡å—ä¼—ï¼š{audience}")
        if purpose:
            prompt_parts.append(f"æ¼”ç¤ºç›®çš„ï¼š{purpose}")
        if style:
            prompt_parts.append(f"è®¾è®¡é£æ ¼ï¼š{style}")

        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¸­çš„å½“å‰æç¤ºè¯ï¼Œè¿›è¡Œå¢å¼º
        if context and context.current_prompt:
            prompt_parts.insert(0, f"åŸºäºåŸå§‹éœ€æ±‚ï¼š{context.current_prompt}")

        # æ·»åŠ é»˜è®¤å»ºè®®
        if len(prompt_parts) < 3:
            prompt_parts.append("è¯·ç”Ÿæˆç»“æ„æ¸…æ™°ã€å†…å®¹ä¸“ä¸šçš„æ¼”ç¤ºæ–‡ç¨¿")

        return "\n".join(prompt_parts)

    def _extract_topic(self, content: str) -> Optional[str]:
        """æå–ä¸»é¢˜"""
        # å°è¯•åŒ¹é…å¸¸è§çš„ä¸»é¢˜æè¿°æ¨¡å¼
        patterns = [
            r"å…³äº(.{2,20}?)(çš„|PPT|å¹»ç¯ç‰‡|æ¼”ç¤º)",
            r"ä¸»é¢˜[æ˜¯ä¸ºï¼š:]\s*(.{2,20})",
            r"åˆ¶ä½œ.{0,5}(.{2,20}?)(PPT|å¹»ç¯ç‰‡)",
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()

        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•æå–å…³é”®åè¯
        if "PPT" in content or "å¹»ç¯ç‰‡" in content:
            # ç®€å•æå–
            words = re.findall(r"[\u4e00-\u9fa5]{2,10}", content)
            for word in words:
                if word not in ["å¹»ç¯ç‰‡", "æ¼”ç¤ºæ–‡ç¨¿", "å¸®æˆ‘", "åˆ¶ä½œ", "ç”Ÿæˆ"]:
                    return str(word)

        return None

    def _extract_audience(self, content: str) -> Optional[str]:
        """æå–å—ä¼—"""
        patterns = [
            r"ç»™(.{2,10}?)(çœ‹|æ±‡æŠ¥|å±•ç¤º)",
            r"å‘(.{2,10}?)(æ±‡æŠ¥|å±•ç¤º)",
            r"å—ä¼—[æ˜¯ä¸ºï¼š:]\s*(.{2,10})",
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()

        return None

    def _extract_purpose(self, content: str) -> Optional[str]:
        """æå–ç›®çš„"""
        patterns = [
            r"ç”¨äº(.{2,10})",
            r"ç›®çš„æ˜¯?(.{2,10})",
            r"å¸Œæœ›.{0,5}(.{2,10})",
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()

        # æ ¹æ®å…³é”®è¯æ¨æ–­ç›®çš„
        if "æ±‡æŠ¥" in content:
            return "å·¥ä½œæ±‡æŠ¥"
        if "é”€å”®" in content or "å®¢æˆ·" in content:
            return "é”€å”®å±•ç¤º"
        if "åŸ¹è®­" in content:
            return "åŸ¹è®­æ•™å­¦"

        return None

    def _extract_style(self, content: str) -> Optional[str]:
        """æå–é£æ ¼"""
        style_keywords = {
            "ç®€çº¦": "ç®€çº¦ç°ä»£",
            "æ­£å¼": "å•†åŠ¡æ­£å¼",
            "å•†åŠ¡": "å•†åŠ¡ä¸“ä¸š",
            "æ´»æ³¼": "æ´»æ³¼ç”ŸåŠ¨",
            "åˆ›æ„": "åˆ›æ„æ–°é¢–",
            "ç§‘æŠ€": "ç§‘æŠ€æ„Ÿ",
        }

        for keyword, style in style_keywords.items():
            if keyword in content:
                return style

        return None

    async def generate_response_stream(
        self,
        messages: List[ChatMessage],
        context: Optional[ChatContext] = None,
    ) -> AsyncIterator[ChatResponseChunk]:
        """
        ç”Ÿæˆæµå¼å“åº”

        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            å“åº”å—
        """
        # ç›´æ¥ä½¿ç”¨çœŸå®å¤§æ¨¡å‹
        async for chunk in self._generate_llm_response_stream(
            messages, context
        ):
            yield chunk

    def _generate_clarification_response(self, intent: IntentAnalysis) -> str:
        """ç”Ÿæˆæ¾„æ¸…ç±»å‹çš„å“åº”"""
        response_parts = ["æˆ‘ç†è§£æ‚¨éœ€è¦å¸®åŠ©ã€‚ä¸ºäº†æ›´å¥½åœ°ååŠ©æ‚¨ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼š\n"]

        for i, question in enumerate(intent.suggested_questions or [], 1):
            response_parts.append(f"{i}. {question}\n")

        if not intent.suggested_questions:
            response_parts.append("æ‚¨å…·ä½“æƒ³åˆ¶ä½œä»€ä¹ˆå†…å®¹çš„ PPT å‘¢ï¼Ÿ")

        return "".join(response_parts)

    def _generate_optimization_response(self, intent: IntentAnalysis) -> str:
        """ç”Ÿæˆä¼˜åŒ–ç±»å‹çš„å“åº”"""
        response = "æ ¹æ®æ‚¨çš„æè¿°ï¼Œæˆ‘å·²ç»ä¸ºæ‚¨ä¼˜åŒ–äº†æç¤ºè¯ï¼š\n\n"

        if intent.suggested_questions:
            response += "å¦å¤–ï¼Œæ‚¨è¿˜å¯ä»¥è€ƒè™‘è¡¥å……ä»¥ä¸‹ä¿¡æ¯ï¼š\n"
            for question in intent.suggested_questions:
                response += f"- {question}\n"

        return response

    def _generate_suggestion_response(self, intent: IntentAnalysis) -> str:
        """ç”Ÿæˆå»ºè®®ç±»å‹çš„å“åº”"""
        response = "æˆ‘æœ‰ä¸€äº›å»ºè®®å¯ä»¥å¸®åŠ©æ‚¨ï¼š\n\n"

        for i, question in enumerate(intent.suggested_questions or [], 1):
            response += f"{i}. {question}\n"

        response += "\nè¯·å‘Šè¯‰æˆ‘æ›´å¤šç»†èŠ‚ï¼Œæˆ‘å¯ä»¥å¸®æ‚¨ä¼˜åŒ–æç¤ºè¯ã€‚"

        return response

    def _generate_general_response(self, messages: List[ChatMessage]) -> str:
        """ç”Ÿæˆä¸€èˆ¬ç±»å‹çš„å“åº”"""
        last_message = self._get_last_user_message(messages)

        if last_message:
            content = last_message.content.lower()

            # ç®€å•çš„å›å¤é€»è¾‘
            if "ä½ å¥½" in content or "æ‚¨å¥½" in content:
                return "æ‚¨å¥½ï¼æˆ‘æ˜¯ AI æç¤ºè¯åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨ä¼˜åŒ– PPT ç”Ÿæˆçš„æç¤ºè¯ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³åˆ¶ä½œä»€ä¹ˆæ ·çš„ PPTï¼Ÿ"

            if "è°¢è°¢" in content or "æ„Ÿè°¢" in content:
                return "ä¸å®¢æ°”ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ã€‚"

            if "å†è§" in content or "æ‹œæ‹œ" in content:
                return "å†è§ï¼ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼"

        # é»˜è®¤å“åº”
        return "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ä¼˜åŒ– PPT ç”Ÿæˆçš„æç¤ºè¯ã€‚è¯·æè¿°æ‚¨æƒ³åˆ¶ä½œçš„ PPT å†…å®¹ï¼Œæˆ‘ä¼šæä¾›ä¸“ä¸šçš„å»ºè®®ã€‚"

    async def _generate_llm_response_stream(
        self,
        messages: List[ChatMessage],
        context: Optional[ChatContext] = None,
    ) -> AsyncIterator[ChatResponseChunk]:
        """
        ä½¿ç”¨çœŸå®å¤§æ¨¡å‹ç”Ÿæˆæµå¼å“åº”

        Args:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            å“åº”å—
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        llm_messages = [{"role": "system", "content": CHAT_SYSTEM_PROMPT}]

        # æ·»åŠ å†å²æ¶ˆæ¯
        for msg in messages:
            role_value = (
                msg.role.value if hasattr(msg.role, "value") else msg.role
            )
            llm_messages.append({"role": role_value, "content": msg.content})

        # åˆ›å»º LLM è¯·æ±‚
        request = LLMRequest(
            messages=llm_messages,
            temperature=settings.chat_ai_temperature,
            max_tokens=settings.chat_ai_max_tokens,
            stream=True,
        )

        # è·å– LLM å®¢æˆ·ç«¯
        client = self._get_llm_client()
        if client is None:
            # å¦‚æœæ— æ³•è·å–å®¢æˆ·ç«¯ï¼Œè¿”å›é”™è¯¯
            yield ChatResponseChunk(
                content="æŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®åé‡è¯•ã€‚",
                is_finished=True,
                has_optimized_prompt=False,
                optimized_prompt=None,
                thinking_content=None,
            )
            return

        # è°ƒç”¨å¤§æ¨¡å‹æµå¼æ¥å£
        full_response = ""

        async for chunk in client.complete_stream(request):
            full_response += chunk.content
            # ä¸å†å®æ—¶è¾“å‡ºï¼Œå…ˆæ”¶é›†å®Œæ•´å“åº”

        # æå–æ€è€ƒå†…å®¹å’Œä¼˜åŒ–åçš„æç¤ºè¯
        thinking_content = self._extract_thinking_content(full_response)
        optimized_prompt = self._extract_optimized_prompt(full_response)

        if optimized_prompt:
            # å¦‚æœæœ‰ä¼˜åŒ–æç¤ºè¯ï¼Œåªè¾“å‡ºå›ºå®šç»“æŸè¯­
            yield ChatResponseChunk(
                content="å¦‚æœæœ‰å“ªé‡Œä¸æ»¡æ„ï¼Œå¯ä»¥ç›´æ¥æå‡ºæ¥ï¼Œæˆ‘å†ä¿®æ”¹ ğŸ˜Š",
                is_finished=False,
                has_optimized_prompt=False,
                optimized_prompt=None,
                thinking_content=thinking_content,
            )
            # ç„¶åè¾“å‡ºæç¤ºè¯å¡ç‰‡
            yield ChatResponseChunk(
                content="",
                is_finished=True,
                has_optimized_prompt=True,
                optimized_prompt=optimized_prompt,
                thinking_content=thinking_content,
            )
        else:
            # æ²¡æœ‰ä¼˜åŒ–æç¤ºè¯æ—¶ï¼Œæ¸…ç†å“åº”ä¸­çš„æ ‡è®°åè¾“å‡º
            clean_response = self._clean_response(full_response)
            yield ChatResponseChunk(
                content=clean_response,
                is_finished=True,
                has_optimized_prompt=False,
                optimized_prompt=None,
                thinking_content=thinking_content,
            )

    def _clean_response(self, response: str) -> str:
        """
        æ¸…ç†å“åº”ä¸­çš„æ ‡è®°

        Args:
            response: åŸå§‹å“åº”å†…å®¹

        Returns:
            æ¸…ç†åçš„å“åº”å†…å®¹
        """
        # ç§»é™¤æ€è€ƒå—
        clean = re.sub(r"\[THINKING_START[^\]]*\]", "", response)
        clean = re.sub(r"\[THINKING_END[^\]]*\]", "", clean)
        # ç§»é™¤æç¤ºè¯å—
        clean = re.sub(
            r"\[PROMPT_START[^\]]*\].*?\[PROMPT_END[^\]]*\]",
            "",
            clean,
            flags=re.DOTALL,
        )
        # æ¸…ç†å¤šä½™ç©ºç™½
        clean = re.sub(r"\n{3,}", "\n\n", clean)
        return clean.strip()

    def _extract_thinking_content(self, response: str) -> Optional[str]:
        """
        ä»å¤§æ¨¡å‹å“åº”ä¸­æå–æ€è€ƒå†…å®¹

        Args:
            response: å¤§æ¨¡å‹çš„å“åº”å†…å®¹

        Returns:
            æ€è€ƒå†…å®¹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œæ”¯æŒæ ‡è®°åè·Ÿå…¶ä»–å­—ç¬¦çš„æƒ…å†µ
        pattern = r"\[THINKING_START[^\]]*\](.*?)\[THINKING_END[^\]]*\]"
        match = re.search(pattern, response, re.DOTALL)
        if match:
            thinking = match.group(1).strip()
            return thinking if thinking else None
        return None

    def _extract_optimized_prompt(self, response: str) -> Optional[str]:
        """
        ä»å¤§æ¨¡å‹å“åº”ä¸­æå–ä¼˜åŒ–åçš„æç¤ºè¯

        Args:
            response: å¤§æ¨¡å‹çš„å“åº”å†…å®¹

        Returns:
            ä¼˜åŒ–åçš„æç¤ºè¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œæ”¯æŒæ ‡è®°åè·Ÿå…¶ä»–å­—ç¬¦çš„æƒ…å†µ
        pattern = r"\[PROMPT_START[^\]]*\](.*?)\[PROMPT_END[^\]]*\]"
        match = re.search(pattern, response, re.DOTALL)
        if match:
            prompt = match.group(1).strip()
            return prompt if prompt else None
        return None


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
chat_service = ChatService()
